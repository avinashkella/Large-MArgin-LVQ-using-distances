%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\makeatletter
\def\fnum@figure{\figurename\thefigure{}}
\makeatother
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\makeatletter
\def\fnum@table{\tablename\thetable{}}
\makeatother
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{LMLVQ using distance Documentation}
\date{Oct 21, 2020}
\release{1.1}
\author{Avinash Maheshwari}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Algorithm Description}
\label{\detokenize{index:algorithm-description}}
Learning Vector Quantization(LVQ) is well-known for Supervised Vector Quantization. Large margin LVQ is to maximize the distance of sample margin or to maximize the distance between decision hyperplane and datapoints.


\section{Pseudo-code}
\label{\detokenize{index:pseudo-code}}\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
Get data with labels e.g. \(x \in X, |X| = n, X \subset R^n\) , where \sphinxstyleemphasis{X} are datapoints and \(c(X) \in C\), where \sphinxstyleemphasis{C} are data labels.

\item {} 
Initialize prototypes with labels e.g. \(w \in R^n\), where \sphinxstyleemphasis{w} are prototypes and \(c(w) \in C\), where \sphinxstyleemphasis{C} are prototype labels.

\item {} 
Calculate Euclidean distance between datapoints and prototypes.

\end{enumerate}
\begin{equation*}
\begin{split}d_{i,j} = d_{E}(x_i , w_j) = \sqrt{\sum (x_i - w_k)^2}\end{split}
\end{equation*}\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{3}
\item {} 
Calculate closest correct matching prototype for every data point and also calculate \(|P_k|\) is the number of data points for which \(w_k\) is the closest prototype with same label.

\item {} 
Calculate \(1_{P_k}\), A vector which has \sphinxstylestrong{1} where data point has closest prototype otherwise zero.

\item {} 
Compute \(A_k\),

\end{enumerate}
\begin{itemize}
\item {} 
The index \(A_k[i, K*i+l]\), should be \sphinxstylestrong{+1} if data point \sphinxstyleemphasis{i} is in \(|P_k|\), i.e. if prototype \sphinxstyleemphasis{k} is the closest prototype to data point \sphinxstyleemphasis{i} with the same label, and if prototype \sphinxstyleemphasis{l} has a different label.

\item {} 
The index \(A_k[i, K*i+k]\) should be \sphinxstylestrong{-1} if datapoint \sphinxstyleemphasis{i} has a different label than prototype \sphinxstyleemphasis{k}.

\item {} 
The index \(A_k[i, K*i+l]\) should be zero in all other cases. So most of \(A_k\) is zero.

\end{itemize}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{6}
\item {} 
Compute the cost function:

\end{enumerate}
\begin{equation*}
\begin{split}E = \min_{\vec{\lambda} \in \mathbb{R}^{m\cdot K}} \hspace{2mm} \frac{1}{2} \vec{\lambda}^T \cdot \left(C \cdot I - \sum_{k=1}^{K} \textbf{A}_{k}^T \cdot \frac{D}{|P_k|} \cdot \textbf{A}_{k}\right)\cdot \vec{\lambda} - \left(\gamma \cdot \vec{1}^T + \sum_{k=1}^{K} \vec{1}_{P_k}^T \cdot \frac{D}{|P_k|} \cdot \textbf{A}_{k}\right) \cdot \vec{\lambda}\end{split}
\end{equation*}\begin{itemize}
\item {} 
such that \(\vec{\lambda} \geq 0\) and  \(\vec{1}^T \cdot \textbf{A}_{k}^T \cdot \vec{\lambda} = 0\), \(\forall k \in \{1,\dots,K\}\)

\end{itemize}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{7}
\item {} 
Finally updates the prototypes:

\end{enumerate}
\begin{equation*}
\begin{split}\lambda(t+1) = \lambda(t) - \eta \frac{\partial E}{\partial w(t)}\end{split}
\end{equation*}

\chapter{Installation Requirements}
\label{\detokenize{index:installation-requirements}}
Following are the basic requirements to run this program:
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
\sphinxhref{www.python.org}{python} with minimum version 3.8.

\item {} 
\sphinxhref{https://numpy.org/install/}{numpy} with minimum version 1.19.0.

\item {} 
\sphinxhref{https://matplotlib.org/users/installing.html}{matplotlib}.

\item {} 
\sphinxhref{https://scikit-learn.org/stable/}{Scikit Learn} .

\end{enumerate}


\section{Execution}
\label{\detokenize{index:execution}}\begin{itemize}
\item {} 
Open the \sphinxstylestrong{lmlvq\_call.py} file and set parameters with margin, prototypes per class and epochs then run it.

\end{itemize}


\chapter{Classes and Functions}
\label{\detokenize{index:module-lmlvq_distance}}\label{\detokenize{index:classes-and-functions}}\index{lmlvq\_distance (module)@\spxentry{lmlvq\_distance}\spxextra{module}}
Created on Thu Oct  1 09:46:20 2020

@author: avinash
\index{LMLVQ (class in lmlvq\_distance)@\spxentry{LMLVQ}\spxextra{class in lmlvq\_distance}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{lmlvq\_distance.}}\sphinxbfcode{\sphinxupquote{LMLVQ}}}{\emph{prototype\_per\_class=1}}{}
Bases: \sphinxcode{\sphinxupquote{object}}

Large margin LVQ is to maximize the distance of sample margin or
to maximize the distance between decision hyperplane and data point.
\begin{description}
\item[{prototype\_per\_class:}] \leavevmode
The number of prototypes per class to be learned.

\end{description}
\index{A\_k() (lmlvq\_distance.LMLVQ method)@\spxentry{A\_k()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.A_k}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{A\_k}}}{\emph{input\_data}, \emph{prototype\_labels}, \emph{w\_plus\_index}}{}
A\_K used to translating the lambda numbers to the beta numbers.

The index A\_k{[}i, K*i+l{]} should be +1 if data point i is in P\_k, i.e.
if prototype k is the closest prototype to data point i with the same
label, \_and\_ if prototype l has a different label.

The index A\_k{[}i, K*i+k{]} should be -1 if datapoint i has a different
label than prototype k.

The index A\_k{[}i, K*i+l{]} should be zero in all other
cases. So most of A\_k is zero.
\begin{description}
\item[{input\_data:}] \leavevmode
A n x m matrix of datapoints.

\item[{prototype\_labels:}] \leavevmode
A n-dimensional vector containing the labels for each
prototype.

\item[{w\_plus\_index:}] \leavevmode
A n-dimensional vector containing the indices for nearest
prototypes to datapoints with same label.

\end{description}
\begin{description}
\item[{a\_k:}] \leavevmode
A N x (N*K) dimentional array where N is data points and K are
prototypes. This is for every prototype

\end{description}

\end{fulllineitems}

\index{beta\_k() (lmlvq\_distance.LMLVQ method)@\spxentry{beta\_k()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.beta_k}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{beta\_k}}}{\emph{lam}, \emph{prototype\_labels}, \emph{ak}, \emph{one\_pk}}{}
Use to compute Euclidean distance between data points and prototypes.

beta\_k = A\_k * lambda + 1\_\{P\_k\}
\begin{description}
\item[{lam:}] \leavevmode
A N*K lambda vector where N is the number of data points and K is
the number of prototypes

\item[{prototype\_labels:}] \leavevmode
A n-dimensional vector containing the labels for each

\item[{ak:}] \leavevmode
A N x (N*K) dimentional array where N is data points and K are
prototypes. This is for every prototype.

\item[{one\_pk:}] \leavevmode
A m-dimensional vector which has 1 for data point has
closest prototype otherwise zero.

\end{description}
\begin{description}
\item[{result:}] \leavevmode
A K x (N x 1) matrix with K is number of prototypes, N is number
of datapoints.

\end{description}

\end{fulllineitems}

\index{cost\_function() (lmlvq\_distance.LMLVQ method)@\spxentry{cost\_function()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.cost_function}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cost\_function}}}{\emph{lam}, \emph{C}, \emph{gamma}, \emph{ak}, \emph{D}, \emph{one\_pk}, \emph{pk}, \emph{prototype\_labels}, \emph{kappa}}{}
Calculate cost function of LMLVQ.

cost function: 0.5 * lambda\textasciicircum{}T * H * lambda - q\textasciicircum{}T * lambda
where H = C*I - sum\_k (A\_k * (D/P\_k) * A\_k)
and q = gamma * 1.T + sum\_k (1\_\{P\_k\} * (D/P\_k) * A\_k)
\begin{description}
\item[{lam:}] \leavevmode
A N*K lambda vector where N is the number of data points and K is
the number of prototypes

\item[{C:}] \leavevmode
The regularization constant.

\item[{gamma:}] \leavevmode
The margin parameter.

\item[{ak:}] \leavevmode
A N x (N*K) dimentional array where N is data points and K are
prototypes. This is for every prototype.

\item[{D:}] \leavevmode
A n x n matrix with Euclidean distance between datapoints.

\item[{one\_pk:}] \leavevmode
A m-dimensional vector which has 1 for data point has
closest prototype otherwise zero.

\item[{pk:}] \leavevmode
A list  of numbers of datapoints for which w\_k is closest
prototype.

\item[{prototype\_labels:}] \leavevmode
A n-dimensional vector containing the labels for each
prototype.

\item[{kappa:}] \leavevmode
A hyperparameter.

\end{description}
\begin{description}
\item[{H:}] \leavevmode
A n x n matrix of result C*I - sum\_k (A\_k * (D/P\_k) * A\_k)

\item[{q:}] \leavevmode
A n-dimensional vector of result:
gamma * 1.T + sum\_k (1\_\{P\_k\} * (D/P\_k) * A\_k)

\item[{cf:}] \leavevmode
A n x n matrix of result of cost function.

\end{description}

\end{fulllineitems}

\index{d\_i\_k() (lmlvq\_distance.LMLVQ method)@\spxentry{d\_i\_k()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.d_i_k}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{d\_i\_k}}}{\emph{input\_data}, \emph{prototype\_labels}, \emph{D}, \emph{beta}}{}
Calculate distance between data points and prototypes.
\begin{description}
\item[{Formula:}] \leavevmode\begin{description}
\item[{d\_\{i,k\} = sum\_j eta\_k{[}j{]} * D{[}i, j{]} / np.sum(beta\_k) - 0.5 *}] \leavevmode
np.dot(beta\_k, np.dot(D, beta\_k)) / (np.sum(beta\_k) ** 2)

\end{description}

\end{description}
\begin{description}
\item[{input\_data:}] \leavevmode
A n x m matrix of datapoints.

\item[{prototype\_labels:}] \leavevmode
A n-dimensional vector containing the labels for each

\item[{D:}] \leavevmode
A n x n matrix with Euclidean distance between datapoints.

\item[{beta:}] \leavevmode
A K x (N x 1) matrix with K is number of prototypes, N is number
of datapoints.

\end{description}
\begin{description}
\item[{dik:}] \leavevmode
A n x m matrix with distance between datapoints and
prototypes.

\end{description}

\end{fulllineitems}

\index{euclidean\_dist() (lmlvq\_distance.LMLVQ method)@\spxentry{euclidean\_dist()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.euclidean_dist}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{euclidean\_dist}}}{\emph{input\_data}, \emph{prototypes}}{}
Calculate squared Euclidean distance between datapoints and
prototypes.
\begin{description}
\item[{input\_data:}] \leavevmode
A n x m matrix of datapoints.

\item[{prototpes:}] \leavevmode
A n x m matrix of prototyes of each class.

\end{description}
\begin{description}
\item[{eu\_dist:}] \leavevmode
A n x m matrix with Euclidean distance between datapoints and
prototypes.

\item[{D:}] \leavevmode
A n x n matrix with Euclidean distance between datapoints.

\end{description}

\end{fulllineitems}

\index{fit() (lmlvq\_distance.LMLVQ method)@\spxentry{fit()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.fit}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit}}}{\emph{input\_data}, \emph{data\_labels}, \emph{learning\_rate}, \emph{epochs}, \emph{margin}, \emph{constant}, \emph{kappa}}{}
Train the Algorithm.
\begin{description}
\item[{input\_data:}] \leavevmode
A n x m matrix of datapoints.

\item[{data\_labels:}] \leavevmode
A n-dimensional vector containing the labels for each
datapoint.

\item[{learning\_rate:}] \leavevmode
The step size.

\item[{epochs:}] \leavevmode
The maximum number of optimization iterations.

\item[{margin:}] \leavevmode
The margin parameter.

\item[{Constant:}] \leavevmode
The regularization constant.

\item[{kappa:}] \leavevmode
A hyperparameter.

\end{description}
\begin{description}
\item[{beta:}] \leavevmode
A K x (N x 1) updated beta matrix with K is number of prototypes,
N is number of datapoints.

\end{description}

\end{fulllineitems}

\index{mod\_Pk() (lmlvq\_distance.LMLVQ method)@\spxentry{mod\_Pk()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.mod_Pk}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{mod\_Pk}}}{\emph{input\_data}, \emph{data\_labels}, \emph{prototype\_labels}, \emph{eu\_dist}}{}
Calculate the number of datapoints for which w\_k is closest prototype.
\begin{description}
\item[{input\_data:}] \leavevmode
A n x m matrix of datapoints.

\item[{data\_labels:}] \leavevmode
A n-dimensional vector containing the labels for each
datapoint.

\item[{prototype\_labels:}] \leavevmode
A n-dimensional vector containing the labels for each
prototype.

\item[{eu\_dist:}] \leavevmode
A n x m matrix with Euclidean distance between datapoints and
prototypes.

\end{description}
\begin{description}
\item[{w\_plus\_index:}] \leavevmode
A n-dimensional vector containing the indices for nearest
prototypes to datapoints with same label.

\item[{pk:}] \leavevmode
A list  of numbers of datapoints for which w\_k is closest
prototype.

\end{description}

\end{fulllineitems}

\index{normalization() (lmlvq\_distance.LMLVQ method)@\spxentry{normalization()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.normalization}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{normalization}}}{\emph{input\_data}}{}
Normalize the data between range 0 and 1.
\begin{description}
\item[{input\_value:}] \leavevmode
A n x m matrix of input data.

\end{description}
\begin{description}
\item[{normalized\_data:}] \leavevmode
A n x m matrix with values between 0 and 1.

\end{description}

\end{fulllineitems}

\index{one\_p\_k() (lmlvq\_distance.LMLVQ method)@\spxentry{one\_p\_k()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.one_p_k}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{one\_p\_k}}}{\emph{input\_data}, \emph{prototype\_labels}, \emph{w\_plus\_index}}{}
A vector which has 1 where data point has closest prototype otherwise
zero.
\begin{description}
\item[{input\_data:}] \leavevmode
A n x m matrix of datapoints.

\item[{prototype\_labels:}] \leavevmode
A n-dimensional vector containing the labels for each
prototype.

\item[{w\_plus\_index:}] \leavevmode
A n-dimensional vector containing the indices for nearest
prototypes to datapoints with same label.

\end{description}
\begin{description}
\item[{pk:}] \leavevmode
A m-dimensional vector which has 1 for data point has
closest prototype otherwise zero.

\end{description}

\end{fulllineitems}

\index{predict() (lmlvq\_distance.LMLVQ method)@\spxentry{predict()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{input\_value}, \emph{input\_data}}{}
Predicts the labels for the data represented by the
given test-to-training distance matrix.
\begin{description}
\item[{input\_value:}] \leavevmode
A n x m matrix of distances from the test to the training
datapoints.

\item[{input\_data:}] \leavevmode
A n x m matrix of datapoints.

\end{description}
\begin{description}
\item[{ylabel:}] \leavevmode
A n-dimensional vector containing the predicted labels for each
datapoint.

\end{description}

\end{fulllineitems}

\index{prt() (lmlvq\_distance.LMLVQ method)@\spxentry{prt()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.prt}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{prt}}}{\emph{input\_data}, \emph{data\_labels}, \emph{prototype\_per\_class}}{}
Calculate prototypes with labels either at mean or randomly depends
on prototypes per class.
\begin{description}
\item[{input\_value:}] \leavevmode
A n x m matrix of datapoints.

\item[{data\_labels:}] \leavevmode
A n-dimensional vector containing the labels for each
datapoint.

\item[{prototypes per class:}] \leavevmode
The number of prototypes per class to be learned. If it is
equal to 1 then prototypes assigned at mean position
else it assigns randolmy.

\end{description}
\begin{description}
\item[{prototype\_labels:}] \leavevmode
A n-dimensional vector containing the labels for each
prototype.

\item[{prototypes:}] \leavevmode
A n x m matrix of prototyes.for training.

\item[{lambda:}] \leavevmode
A n*m array of zeros

\end{description}

\end{fulllineitems}

\index{prt\_labels (lmlvq\_distance.LMLVQ attribute)@\spxentry{prt\_labels}\spxextra{lmlvq\_distance.LMLVQ attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.prt_labels}}\pysigline{\sphinxbfcode{\sphinxupquote{prt\_labels}}\sphinxbfcode{\sphinxupquote{ = array({[}{]}, dtype=float64)}}}
\end{fulllineitems}

\index{update() (lmlvq\_distance.LMLVQ method)@\spxentry{update()}\spxextra{lmlvq\_distance.LMLVQ method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.update}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update}}}{\emph{lam}, \emph{H}, \emph{q}, \emph{learning\_rate}}{}
To update the lambda vector.

gradient = H * lambda - q
lambda(t+1) = lambda - learning rate * gradient
\begin{description}
\item[{lam:}] \leavevmode
A N*K lambda vector where N is the number of data points and K is
the number of prototypes

\item[{H:}] \leavevmode
A n x n matrix of result C*I - sum\_k (A\_k * (D/P\_k) * A\_k)

\item[{q:}] \leavevmode
A n-dimensional vector of result:
gamma * 1.T + sum\_k (1\_\{P\_k\} * (D/P\_k) * A\_k)

\item[{learning\_rate:}] \leavevmode
The step size.

\end{description}
\begin{description}
\item[{lam\_update:}] \leavevmode
A N*K updated lambda vector where N is the number of data points
and K is the number of prototypes.

\end{description}

\end{fulllineitems}

\index{update\_beta (lmlvq\_distance.LMLVQ attribute)@\spxentry{update\_beta}\spxextra{lmlvq\_distance.LMLVQ attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:lmlvq_distance.LMLVQ.update_beta}}\pysigline{\sphinxbfcode{\sphinxupquote{update\_beta}}\sphinxbfcode{\sphinxupquote{ = array({[}{]}, dtype=float64)}}}
\end{fulllineitems}


\end{fulllineitems}



\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{l}
\item\relax\sphinxstyleindexentry{lmlvq\_distance}\sphinxstyleindexpageref{index:\detokenize{module-lmlvq_distance}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}